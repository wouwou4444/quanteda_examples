---
title: "NLP"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages(c("ggplot2", "e1071", "caret", "quanteda", "irlba", "randomForest"))

```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r }
setwd("d:/valbuz/OneDrive/Coursera2/R/quanteda_examples")
spam.raw <- read.csv("spam.csv", stringsAsFactors = FALSE)
spam.raw <- spam.raw[, 1:2]

names(spam.raw) <- c("label", "text")
head(spam.raw)
```

## Check if incomplete values

check if any NA value

```{r }
length(which(!complete.cases(spam.raw)))
```

## Explore Data

```{r}
spam.raw$label <- as.factor(spam.raw$label)
base::table(spam.raw$label)
prop.table(base::table(spam.raw$label))
```


### Add length of text

```{r}
spam.raw$textLength <- nchar(spam.raw$text)
head(spam.raw)
summary(spam.raw$textLength)
```


### draw a histogram of label by size

```{r}
library(ggplot2)

ggplot(spam.raw, aes(x=textLength, fill=label)) +
  theme_minimal() +
  geom_histogram( binwidth = 5) +
  labs(title = "Distribution of text lengths with class labels")
```

### Split Data in training set and test set

    Use caret package
    
Use the caret package
```{r}
library(caret)
# help(package="caret")

set.seed(32984)
indexes <- createDataPartition(spam.raw$label, times = 1,
                               p = 0.7, list = FALSE)
train <- spam.raw[indexes,]
test <- spam.raw[-indexes,]

table(train$label)
prop.table(table(train$label))
table(test$label)
prop.table(table(test$label))
```


### Text pre-processing

    tokenization
    lowercase
    remove stopwords
    stemming

```{r}
library(quanteda)
train.tokens <- tokens(train$text, what = "word",
                       remove_numbers = TRUE, remove_punct = TRUE,
                       remove_symbols = TRUE, remove_hyphens = TRUE)
train$text[357]
train.tokens[[357]]
train.tokens <-  tokens_tolower(train.tokens)
train.tokens[[357]]
train.tokens <- tokens_select(train.tokens, stopwords(language = "english"),
                              selection = "remove")
train.tokens[[357]]
train.tokens <- tokens_wordstem(train.tokens, language = "english")
train.tokens[[357]]

```


### Create Document Frequency Matrix

```{r}
train.tokens.dfm <- dfm(train.tokens, tolower = FALSE)
train.tokens.matrix <- as.matrix(train.tokens.dfm)
head(train.tokens.matrix[1:6, 1:50])

dim(train.tokens.matrix)
colnames(train.tokens.matrix)[1:50]

```

### Model


### Prepare one single matrix with label in first column and features in other columns

    Use cbind to add in column the labels (spam, ham)
    transform in a dataframe
    the  tokens for the dataframe may not be compatible to be column names
    transform names in compatible names

```{r}
train.tokens.df <- cbind( label = train$label, convert(train.tokens.dfm, to = "data.frame"))
dim(train.tokens.df)
names(train.tokens.df)[c(146, 148, 235, 238)]

names(train.tokens.df) <- make.names(names(train.tokens.df))

```


### Train the first model

### Create stratified folds for cross-validation repeated 3 times

```{r}
set.seed(48743)
cv.folds <- createMultiFolds(train$label, k=10, times = 3)

cv.cntrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, index = cv.folds)
```



```{r}

# install.packages("doSNOW")
library(doSNOW)

start.time <- Sys.time()
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)

rpart.cv.1 <- train(label ~ . , data = train.tokens.df, method = "rpart",
                    trControl = cv.cntrl, tuneLength = 7)
stopCluster(cl)
total.time <- Sys.time() - start.time
total.time
```


## TF-IDF 

```{r}
term.frequency <- function (row) {
  row / sum (row)
}
train.tokens.tf <- apply(train.tokens.matrix, 1, term.frequency)


inverse.doc.freq <- function(col) {
  doc.size <- length (col)
  doc.term.count <- length(which(col > 0))
  log10(doc.size/doc.term.count)
}

tf.idf <- function (tf, idf) {
  tf * idf
}

train.tokens.idf <- apply(train.tokens.matrix, 2, inverse.doc.freq)
str(train.tokens.idf)

train.tokens.tfidf <- apply(train.tokens.tf, 2, tf.idf, idf = train.tokens.idf)
dim(train.tokens.tfidf)
# View'train.tokens.tfidf[1:25,1:25])

train.tokens.tfidf <- t(train.tokens.tfidf)
dim(train.tokens.tfidf)
# View'train.tokens.tfidf[1:25,1:25])


```

```{r}
incomplete.cases <- which(!complete.cases(train.tokens.tfidf))
train$text[incomplete.cases]

train.tokens.tfidf[incomplete.cases,] <- rep(0.0, ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
length(which(!complete.cases(train.tokens.tfidf)))

```


### Check that there is no degenerative cases


